{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Time series forecasting\n",
        "\n",
        "## Score: .1299"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['LOKY_MAX_CPU_COUNT'] = str(os.cpu_count() or 4)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths: Kaggle or local\n",
        "if os.path.exists('/kaggle/input/ts-forecasting/train.parquet'):\n",
        "    TRAIN_PATH = '/kaggle/input/ts-forecasting/train.parquet'\n",
        "    TEST_PATH = '/kaggle/input/ts-forecasting/test.parquet'\n",
        "else:\n",
        "    TRAIN_PATH = 'ts-forecasting/train.parquet'\n",
        "    TEST_PATH = 'ts-forecasting/test.parquet'\n",
        "\n",
        "VAL_THRESHOLD = 3600\n",
        "# If LB is 0 with id order, set False to write in parquet row order instead\n",
        "SUBMISSION_BY_ID_ORDER = True\n",
        "\n",
        "\n",
        "def weighted_rmse_score(y_target, y_pred, w):\n",
        "    \"\"\"Competition metric. Higher is better.\"\"\"\n",
        "    y_target, y_pred, w = np.array(y_target), np.array(y_pred), np.array(w)\n",
        "    denom = np.sum(w * (y_target ** 2))\n",
        "    if denom <= 0:\n",
        "        return 0.0\n",
        "    numerator = np.sum(w * ((y_target - y_pred) ** 2))\n",
        "    ratio = numerator / denom\n",
        "    return float(np.sqrt(1.0 - np.clip(ratio, 0.0, 1.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train stats computed.\n"
          ]
        }
      ],
      "source": [
        "# Train stats from data with ts_index <= val_threshold (no look-forward)\n",
        "temp = pd.read_parquet(TRAIN_PATH, columns=['sub_category', 'sub_code', 'y_target', 'ts_index'])\n",
        "train_only = temp[temp['ts_index'] <= VAL_THRESHOLD]\n",
        "train_stats = {\n",
        "    'sub_category': train_only.groupby('sub_category')['y_target'].mean().to_dict(),\n",
        "    'sub_code': train_only.groupby('sub_code')['y_target'].mean().to_dict(),\n",
        "    'global_mean': train_only['y_target'].mean()\n",
        "}\n",
        "del temp, train_only\n",
        "gc.collect()\n",
        "print('Train stats computed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_context_features(data, enc_stats=None):\n",
        "    \"\"\"Context-aware features: encoded categoricals, interactions, lags, temporal cycle.\"\"\"\n",
        "    x = data.copy()\n",
        "    group_cols = ['code', 'sub_code', 'sub_category', 'horizon']\n",
        "    x = x.sort_values(group_cols + ['ts_index'])\n",
        "\n",
        "    if enc_stats is not None:\n",
        "        for c in ['sub_category', 'sub_code']:\n",
        "            x[c + '_enc'] = x[c].map(enc_stats[c]).fillna(enc_stats['global_mean']).astype(np.float32)\n",
        "\n",
        "    # Interaction features\n",
        "    if 'feature_al' in x.columns and 'feature_am' in x.columns:\n",
        "        x['d_al_am'] = (x['feature_al'] - x['feature_am']).astype(np.float32)\n",
        "        x['r_al_am'] = (x['feature_al'] / (x['feature_am'] + 1e-7)).astype(np.float32)\n",
        "    if 'feature_cg' in x.columns and 'feature_by' in x.columns:\n",
        "        x['d_cg_by'] = (x['feature_cg'] - x['feature_by']).astype(np.float32)\n",
        "\n",
        "    top_features = ['feature_al', 'feature_am', 'feature_cg', 'feature_by', 'feature_s']\n",
        "    for col in top_features:\n",
        "        if col not in x.columns:\n",
        "            continue\n",
        "        for lag in [1, 3, 10]:\n",
        "            x[f'{col}_lag{lag}'] = x.groupby(group_cols)[col].shift(lag).astype(np.float32)\n",
        "        x[f'{col}_diff1'] = x.groupby(group_cols)[col].diff(1).astype(np.float32)\n",
        "        for window in [5, 10]:\n",
        "            x[f'{col}_roll{window}'] = x.groupby(group_cols)[col].transform(\n",
        "                lambda s: s.rolling(window, min_periods=1).mean()\n",
        "            ).astype(np.float32)\n",
        "            x[f'{col}_rollstd{window}'] = x.groupby(group_cols)[col].transform(\n",
        "                lambda s: s.rolling(window, min_periods=1).std()\n",
        "            ).astype(np.float32)\n",
        "        x[f'{col}_ewm5'] = x.groupby(group_cols)[col].transform(\n",
        "            lambda s: s.ewm(span=5, adjust=False).mean()\n",
        "        ).astype(np.float32)\n",
        "\n",
        "    # Cross-sectional z-score (within ts_index, horizon) for key features; safe for test\n",
        "    for col in ['feature_al', 'feature_am', 'feature_cg', 'feature_by']:\n",
        "        if col not in x.columns:\n",
        "            continue\n",
        "        grp = x.groupby(['ts_index', 'horizon'])[col]\n",
        "        mu_cs = grp.transform('mean').astype(np.float32)\n",
        "        std_cs = grp.transform('std').astype(np.float32).replace(0, np.nan)\n",
        "        x[f'{col}_cs_z'] = ((x[col] - mu_cs) / std_cs.fillna(1e-3)).fillna(0).astype(np.float32)\n",
        "    x['horizon_num'] = x['horizon'].astype(np.float32)\n",
        "\n",
        "    x['t_cycle'] = np.sin(2 * np.pi * x['ts_index'].astype(np.float32) / 100).astype(np.float32)\n",
        "    return x\n",
        "\n",
        "\n",
        "LGB_CFG = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.015,\n",
        "    'n_estimators': 4000,\n",
        "    'num_leaves': 80,\n",
        "    'min_child_samples': 200,\n",
        "    'feature_fraction': 0.6,\n",
        "    'bagging_fraction': 0.7,\n",
        "    'bagging_freq': 5,\n",
        "    'lambda_l1': 0.1,\n",
        "    'lambda_l2': 10.0,\n",
        "    'verbosity': -1,\n",
        "}\n",
        "SEEDS = (42, 2024, 7, 11, 999)\n",
        "N_SEEDS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Training horizon = 1\n",
            "Horizon 1 Score: 0.10435\n",
            "\n",
            ">>> Training horizon = 3\n",
            "Horizon 3 Score: 0.00000\n",
            "\n",
            ">>> Training horizon = 10\n",
            "Horizon 10 Score: 0.00000\n",
            "\n",
            ">>> Training horizon = 25\n",
            "Horizon 25 Score: 0.09142\n"
          ]
        }
      ],
      "source": [
        "forecast_windows = [1, 3, 10, 25]\n",
        "test_outputs = []\n",
        "cv_cache = {'y': [], 'pred': [], 'wt': []}\n",
        "\n",
        "for hz in forecast_windows:\n",
        "    print(f'\\n>>> Training horizon = {hz}')\n",
        "    tr_df = build_context_features(\n",
        "        pd.read_parquet(TRAIN_PATH).query(f'horizon == {hz}'),\n",
        "        train_stats\n",
        "    )\n",
        "    te_df = build_context_features(\n",
        "        pd.read_parquet(TEST_PATH).query(f'horizon == {hz}'),\n",
        "        train_stats\n",
        "    )\n",
        "\n",
        "    feature_cols = [\n",
        "        c for c in tr_df.columns\n",
        "        if c not in {'id', 'code', 'sub_code', 'sub_category', 'horizon', 'ts_index', 'weight', 'y_target'}\n",
        "    ]\n",
        "    for c in feature_cols:\n",
        "        tr_df[c] = tr_df[c].fillna(0).astype(np.float32)\n",
        "        te_df[c] = te_df[c].fillna(0).astype(np.float32)\n",
        "\n",
        "    fit_mask = tr_df['ts_index'] <= VAL_THRESHOLD\n",
        "    val_mask = ~fit_mask\n",
        "\n",
        "    X_fit = tr_df.loc[fit_mask, feature_cols]\n",
        "    y_fit = tr_df.loc[fit_mask, 'y_target']\n",
        "    w_fit = tr_df.loc[fit_mask, 'weight']\n",
        "    X_hold = tr_df.loc[val_mask, feature_cols]\n",
        "    y_hold = tr_df.loc[val_mask, 'y_target']\n",
        "    w_hold = tr_df.loc[val_mask, 'weight']\n",
        "\n",
        "    val_pred = np.zeros(len(y_hold), dtype=np.float64)\n",
        "    tst_pred = np.zeros(len(te_df), dtype=np.float64)\n",
        "\n",
        "    cfg = dict(LGB_CFG)\n",
        "    if hz in (10, 25):\n",
        "        cfg = {**cfg, 'learning_rate': 0.01, 'n_estimators': 6500, 'num_leaves': 96,\n",
        "               'min_child_samples': 250, 'lambda_l2': 28.0}\n",
        "\n",
        "    for seed in SEEDS:\n",
        "        mdl = lgb.LGBMRegressor(**cfg, random_state=seed, n_jobs=-1)\n",
        "        mdl.fit(\n",
        "            X_fit, y_fit,\n",
        "            sample_weight=w_fit,\n",
        "            eval_set=[(X_hold, y_hold)],\n",
        "            eval_sample_weight=[w_hold],\n",
        "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
        "        )\n",
        "        val_pred += mdl.predict(X_hold).astype(np.float64) / N_SEEDS\n",
        "        tst_pred += mdl.predict(te_df[feature_cols]).astype(np.float64) / N_SEEDS\n",
        "        del mdl\n",
        "        gc.collect()\n",
        "\n",
        "    print(f'Horizon {hz} Score: {weighted_rmse_score(y_hold, val_pred, w_hold):.5f}')\n",
        "\n",
        "    cv_cache['y'].extend(y_hold.tolist())\n",
        "    cv_cache['pred'].extend(val_pred.tolist())\n",
        "    cv_cache['wt'].extend(w_hold.tolist())\n",
        "\n",
        "    test_outputs.append(pd.DataFrame({'id': te_df['id'].values, 'prediction': tst_pred}))\n",
        "\n",
        "    del tr_df, te_df, X_fit, y_fit, w_fit, X_hold, y_hold, w_hold\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "FINAL AGGREGATE SCORE: 0.070073\n",
            "========================================\n",
            "Submission predictions: min=-9.7705, max=0.8772, mean=-0.1347, std=0.5976\n",
            "WARNING: Prediction std is very small vs train (~32.5). This can cause LB 0 (ratio >= 1).\n",
            "Submission shape: (1447107, 2) (rows in id order)\n",
            "First 10 rows:\n",
            "                                       id  prediction\n",
            "0  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4175   -0.009167\n",
            "1  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4176   -0.011500\n",
            "2  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4177   -0.011500\n",
            "3  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4178   -0.011500\n",
            "4  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4179   -0.011500\n",
            "5  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4180   -0.011500\n",
            "6  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4182   -0.011500\n",
            "7  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4183   -0.011500\n",
            "8  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4184   -0.011500\n",
            "9  10BAVIDU__07YQ9WA4__DPPUO5X2__10__4185   -0.011500\n"
          ]
        }
      ],
      "source": [
        "final_metric = weighted_rmse_score(cv_cache['y'], cv_cache['pred'], cv_cache['wt'])\n",
        "print(f\"\\n{'='*40}\\nFINAL AGGREGATE SCORE: {final_metric:.6f}\\n{'='*40}\")\n",
        "\n",
        "# Submission: write in deterministic ID order + UTF-8 so grader gets same format everywhere\n",
        "test_full = pd.read_parquet(TEST_PATH)\n",
        "pred_dict = {}\n",
        "for df in test_outputs:\n",
        "    for i in range(len(df)):\n",
        "        k = str(df['id'].iloc[i]).strip()\n",
        "        pred_dict[k] = float(df['prediction'].iloc[i])\n",
        "\n",
        "test_ids = test_full['id'].astype(str).str.strip()\n",
        "missing = test_ids[~test_ids.isin(pred_dict)].unique()\n",
        "if len(missing) > 0:\n",
        "    raise RuntimeError(f\"{len(missing)} test ids not in pred_dict (e.g. {list(missing[:3])})\")\n",
        "if len(pred_dict) != len(test_full):\n",
        "    raise RuntimeError(f\"pred_dict has {len(pred_dict)} keys but test has {len(test_full)} rows\")\n",
        "\n",
        "# Fix: deterministic row order. Id order works if grader merges by id or test is id-sorted.\n",
        "if SUBMISSION_BY_ID_ORDER:\n",
        "    test_out = test_full.sort_values(by='id', key=lambda col: col.astype(str)).reset_index(drop=True)\n",
        "else:\n",
        "    test_out = test_full.reset_index(drop=True)\n",
        "pred_vals = np.array([pred_dict[str(test_out['id'].iloc[i]).strip()] for i in range(len(test_out))])\n",
        "if np.std(pred_vals) < 1e-6:\n",
        "    raise RuntimeError(f\"Predictions nearly constant (std={np.std(pred_vals):.2e}). Fix training before submitting.\")\n",
        "if np.all(np.abs(pred_vals) < 1e-6):\n",
        "    raise RuntimeError(\"Predictions all ~0. Fix training before submitting.\")\n",
        "# Scale check: train y_target has mean ~-0.67, std ~32.5. If submission preds are all small (e.g. std < 1), ratio can be >= 1 -> LB 0.\n",
        "print(f\"Submission predictions: min={pred_vals.min():.4f}, max={pred_vals.max():.4f}, mean={pred_vals.mean():.4f}, std={pred_vals.std():.4f}\")\n",
        "if pred_vals.std() < 2.0:\n",
        "    print(\"WARNING: Prediction std is very small vs train (~32.5). This can cause LB 0 (ratio >= 1).\")\n",
        "\n",
        "with open('submission.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    f.write('id,prediction\\n')\n",
        "    for i in range(len(test_out)):\n",
        "        id_val = test_out['id'].iloc[i]\n",
        "        key = str(id_val).strip()\n",
        "        pred = pred_dict[key]\n",
        "        if np.isnan(pred):\n",
        "            raise RuntimeError(f\"NaN prediction for id {id_val}\")\n",
        "        f.write(f\"{id_val},{pred:.10f}\\n\")\n",
        "\n",
        "submission = pd.read_csv('submission.csv', encoding='utf-8')\n",
        "print(f'Submission shape: {submission.shape} (rows in id order)')\n",
        "print('First 10 rows:')\n",
        "print(submission.head(10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
